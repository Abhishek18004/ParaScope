{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9409f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting import-ipynb\n",
      "  Downloading import_ipynb-0.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: IPython in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from import-ipynb) (9.3.0)\n",
      "Collecting nbformat (from import-ipynb)\n",
      "  Downloading nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from IPython->import-ipynb) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from IPython->import-ipynb) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from IPython->import-ipynb) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from IPython->import-ipynb) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from IPython->import-ipynb) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from IPython->import-ipynb) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from IPython->import-ipynb) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from IPython->import-ipynb) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from IPython->import-ipynb) (5.14.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from IPython->import-ipynb) (4.14.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->IPython->import-ipynb) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from jedi>=0.16->IPython->import-ipynb) (0.8.4)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat->import-ipynb)\n",
      "  Downloading fastjsonschema-2.21.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from nbformat->import-ipynb) (4.24.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from nbformat->import-ipynb) (5.8.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.25.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (4.3.8)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (310)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from stack_data->IPython->import-ipynb) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from stack_data->IPython->import-ipynb) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\hp\\documents\\projects\\parascope\\.venv\\lib\\site-packages (from stack_data->IPython->import-ipynb) (0.2.3)\n",
      "Downloading import_ipynb-0.2-py3-none-any.whl (4.0 kB)\n",
      "Downloading nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Downloading fastjsonschema-2.21.1-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: fastjsonschema, nbformat, import-ipynb\n",
      "\n",
      "   ---------------------------------------- 0/3 [fastjsonschema]\n",
      "   ---------------------------------------- 0/3 [fastjsonschema]\n",
      "   ------------- -------------------------- 1/3 [nbformat]\n",
      "   ------------- -------------------------- 1/3 [nbformat]\n",
      "   ------------- -------------------------- 1/3 [nbformat]\n",
      "   ------------- -------------------------- 1/3 [nbformat]\n",
      "   ------------- -------------------------- 1/3 [nbformat]\n",
      "   ------------- -------------------------- 1/3 [nbformat]\n",
      "   ------------- -------------------------- 1/3 [nbformat]\n",
      "   ------------- -------------------------- 1/3 [nbformat]\n",
      "   ------------- -------------------------- 1/3 [nbformat]\n",
      "   ------------- -------------------------- 1/3 [nbformat]\n",
      "   ------------- -------------------------- 1/3 [nbformat]\n",
      "   ---------------------------------------- 3/3 [import-ipynb]\n",
      "\n",
      "Successfully installed fastjsonschema-2.21.1 import-ipynb-0.2 nbformat-5.10.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import import_ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c9eda28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the full module\n",
    "import input.text_input as text_input\n",
    "import app.summarizer as summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a4a29473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleSpec(name='app.summarizer', loader=<import_ipynb.NotebookLoader object at 0x0000025A8C4ED1D0>, origin='c:\\\\Users\\\\HP\\\\Documents\\\\Projects\\\\ParaScope\\\\academic_summarizer\\\\app\\\\summarizer.ipynb')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Reload the module\n",
    "importlib.reload(text_input)\n",
    "importlib.reload(summarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Re-import the updated functions\n",
    "from input.text_input import preprocess_text, validate_text\n",
    "from app.summarizer import summarize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4ec41b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"\n",
    "Deep learning models have significantly outperformed traditional machine learning algorithm in tasks such as image \n",
    "recognition, natural language processing, and speech recognition. \n",
    "Recent studies explore transformer-based architectures for sequence-to-sequence learning. \n",
    "For instance, the self-attention mechanism in transformers has led to breakthroughs in natural language understanding \n",
    "tasks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a4fb4e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Text: deep learning models have significantly outperformed traditional machine learning algorithm in tasks such as image recognition, natural language processing, and speech recognition. recent studies expl\n"
     ]
    }
   ],
   "source": [
    "valid = validate_text(raw_text)\n",
    "if not valid[\"valid\"]:\n",
    "    print(\"Validation failed:\", valid[\"error\"])\n",
    "else:\n",
    "    cleaned = preprocess_text(raw_text)\n",
    "    print(\"Preprocessed Text:\", cleaned[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Output ===\n",
      " ### Summary\n",
      "\n",
      "Deep learning, a subset of machine learning, has proven superior to traditional methods in various complex tasks like image, language, and speech processing. Recent research focuses on transformer architectures, particularly for sequence-to-sequence learning â€“ where models learn to convert one sequence of data (e.g., a sentence in English) into another (e.g., the same sentence in French). A key component of transformers is the self-attention mechanism, allowing the model to weigh the importance of different parts of the input sequence when processing it. This self-attention breakthrough has led to marked improvements, especially in natural language understanding. Unlike previous methods that processed sequences sequentially, self-attention allows for parallel processing and captures long-range dependencies more effectively. The ability to understand context, nuance, and relationships between words or elements within a sequence has significantly improved the performance of models across various applications. The transformer architecture and self-attention are now fundamental to modern NLP and are actively being explored and adapted for other domains, driving the state-of-the-art in many AI applications.\n",
      "\n",
      "### Glossary\n",
      "\n",
      "*   **Machine Learning:** A type of artificial intelligence that allows computers to learn from data without being explicitly programmed.\n",
      "*   **Deep Learning:** A subfield of machine learning that uses artificial neural networks with multiple layers to analyze data.\n",
      "*   **Sequence-to-Sequence Learning:** A type of machine learning where a model learns to convert one sequence of data into another sequence.\n",
      "*   **Natural Language Processing (NLP):** A field of computer science focused on enabling computers to understand and process human language.\n",
      "*   **Natural Language Understanding (NLU):** A subfield of NLP concerned with enabling computers to understand the meaning of human language.\n",
      "*   **Self-Attention Mechanism:** A mechanism within transformer models that allows the model to weigh the importance of different parts of the input sequence when processing it.\n",
      "*   **Transformer-Based Architectures:** Neural network architectures that rely on the self-attention mechanism and are particularly effective for sequence-to-sequence learning tasks.\n",
      "\n",
      "### Questions\n",
      "\n",
      "1.  The text highlights the success of transformers in NLP. What inherent limitation of previous sequence processing models (like recurrent neural networks) does the self-attention mechanism overcome, leading to these improvements?\n",
      "\n",
      "    *   **Answer:** Previous models processed sequences sequentially, making it difficult to capture long-range dependencies and leading to vanishing gradients. Self-attention allows for parallel processing and focuses on the relevance of each input element to all others, mitigating these issues.\n",
      "\n",
      "2.  Beyond NLP, where else might transformer-based architectures and self-attention mechanisms be beneficial, and why? Provide a specific example with a rationale.\n",
      "\n",
      "    *   **Answer:** They could be beneficial in time-series analysis, such as predicting stock prices or weather patterns. This is because time-series data is sequential and has complex dependencies that can be captured by self-attention.\n"
     ]
    }
   ],
   "source": [
    "if valid[\"valid\"]:\n",
    "    result = summarize_text(cleaned, mode=\"full\")\n",
    "    if result[\"success\"]:\n",
    "        print(\"=== Final Output ===\\n\", result[\"output\"])\n",
    "    else:\n",
    "        print(\"API Error:\", result[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
